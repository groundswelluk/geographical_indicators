{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HQldgb0_HYf"
      },
      "source": [
        "# Creating green and blue space indicators from Sentinel-2 data\n",
        "\n",
        "The notebook will estimate the following statistics for each household, as defined as the Unique Property Reference Number (UPRN), across Cheshire and Merseyside:\n",
        "* Normalised Difference Vegetation Index (NDVI)\n",
        "* Enhanced Vegetation Index (EVI)\n",
        "* Normalised Water Difference Idex (NWDI)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wtkH4vC_-LE"
      },
      "source": [
        "## Set up the environment\n",
        "\n",
        "We will use Google Earth Engine (GEE) to access Sentinel-2 images. The strength of using GEE is that we can store and process all the images in the cloud, saving space. Let's start with installing GEE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBOUh254_zIB"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# ! pip install earthengine-api # Comes already installed in Colab so leave here for local running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crmiuqNIBc7H"
      },
      "source": [
        "Next we need to set up Python for the neccessary packages and link the notebook to our set up Groundswell GEE project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJVNk8fzBqrB"
      },
      "outputs": [],
      "source": [
        "# Load required libaries\n",
        "import ee\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import time\n",
        "from shapely.geometry import mapping\n",
        "\n",
        "# Set up Google Earth Engine (GEE) module\n",
        "ee.Authenticate(auth_mode = \"colab\") # Links GEE to your Google Account and defines that working in Colab (can change to 'localhost' if working on a local machine)\n",
        "ee.Initialize(project = \"ee-groundswelluk\") # Link to the registered project within GEE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY-pfT9OBrPH"
      },
      "source": [
        "## Process images\n",
        "The first key processing task that we need to undertake is to access the satellite imagery and process it into the neccessary format for generating our indicators. As such, we need to: (i) select which satellite we want to use (Sentinel-2), (ii) define the region and extent of images we need (Cheshire and Merseyside), (iii) select the time period (2024) of interest, (iv) remove any clouds from images so that they do not affect any values, (v) extract the information that we want to extract from the images (NDVI, EVI and NWDI), and (vi) take the median value for the whole time period (i.e., combine the information across multiple images).\n",
        "\n",
        "From my testing, it is more computationally efficient to divide our processing by Local Authority rather than trying to do it for the whole region in one go. So below, we will create and process the image for each Local Authority seperately and then save these to Google Earth Engine. We will link the image values to the household information later.\n",
        "\n",
        "As a rough time guide, it takes ~7 minutes to process all of the images for Liverpool in 2023 in Google Colab / Earth Engine, so should take 35-40 minutes in total."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define bounding box geometries for each area\n",
        "liverpool_bbox = ee.Geometry.Rectangle([-3.012314, 53.324927, -2.808037, 53.479261])\n",
        "sefton_bbox = ee.Geometry.Rectangle([-3.123550, 53.433674, -2.881851, 53.686949])\n",
        "knowsley_bbox = ee.Geometry.Rectangle([-2.940216, 53.313237, -2.682724, 53.512143])\n",
        "wirral_bbox = ee.Geometry.Rectangle([-3.23, 53.3, -2.918587, 53.444104])\n",
        "halton_bbox = ee.Geometry.Rectangle([-2.827263, 53.301954, -2.594833, 53.402982])\n",
        "warrington_bbox = ee.Geometry.Rectangle([-2.698860, 53.316518, -2.455444, 53.482325])\n",
        "chester_cheshire_west_bbox = ee.Geometry.Rectangle([-3.124237, 52.964770, -2.327042, 53.326772])\n",
        "cheshire_east_bbox = ee.Geometry.Rectangle([-2.76, 52.921323, -1.976166, 53.423446])\n",
        "st_helens_bbox = ee.Geometry.Rectangle([-2.845459, 53.351781, -2.526855, 53.555810])\n",
        "\n",
        "# List of bounding boxes\n",
        "areas = [\n",
        "    ('Liverpool', liverpool_bbox),\n",
        "    ('Sefton', sefton_bbox),\n",
        "    ('Knowsley', knowsley_bbox),\n",
        "    ('Wirral', wirral_bbox),\n",
        "    ('Halton', halton_bbox),\n",
        "    ('Warrington', warrington_bbox),\n",
        "    ('Chester_and_Cheshire_West', chester_cheshire_west_bbox),\n",
        "    ('Cheshire_East', cheshire_east_bbox),\n",
        "    ('St_Helens', st_helens_bbox)\n",
        "]\n",
        "\n",
        "# Cloud masking function for Sentinel-2\n",
        "def maskS2clouds(image):\n",
        "    qa = image.select('QA60')\n",
        "    cloudBitMask = ee.Number(2).pow(10).int()\n",
        "    cirrusBitMask = ee.Number(2).pow(11).int()\n",
        "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
        "    return image.updateMask(mask).divide(10000)\n",
        "\n",
        "# Load Sentinel-2 surface reflectance data\n",
        "collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "    .filterDate('2024-01-01', '2024-07-02') \\\n",
        "    .map(maskS2clouds)\n",
        "\n",
        "# Calculate NDVI, EVI, and NDWI\n",
        "def addIndices(image):\n",
        "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "    evi = image.expression(\n",
        "        '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n",
        "            'NIR': image.select('B8'),\n",
        "            'RED': image.select('B4'),\n",
        "            'BLUE': image.select('B2')\n",
        "        }).rename('EVI')\n",
        "    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
        "    return image.addBands([ndvi, evi, ndwi])\n",
        "\n",
        "# Iterate over each area, calculate median value and export the data to Earth Engine assets\n",
        "for area_name, bbox in areas:\n",
        "    print(f'Processing {area_name}...')\n",
        "\n",
        "    # Filter collection by bounding box\n",
        "    area_collection = collection.filterBounds(bbox)\n",
        "\n",
        "    # Calculate indices\n",
        "    dataset = area_collection.map(addIndices)\n",
        "\n",
        "    # Calculate median value for each index for the time period within the bounding box\n",
        "    median = dataset.median().clip(bbox)\n",
        "\n",
        "    # Select the bands of interest\n",
        "    bands = median.select(['NDVI', 'EVI', 'NDWI'])\n",
        "\n",
        "    # Export the data to an Earth Engine asset\n",
        "    task = ee.batch.Export.image.toAsset(\n",
        "        image=bands,\n",
        "        description=f'{area_name}_Vegetation_Water_Indices',\n",
        "        assetId=f'projects/ee-groundswelluk/assets/2024/{area_name}_Vegetation_Water_Indices', # Where to save on earth engine\n",
        "        region=bbox,\n",
        "        scale=10,\n",
        "        maxPixels=1e13\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "    # Monitor the task (let know progress or print any errors)\n",
        "    def monitor_task(task):\n",
        "        while task.active():\n",
        "            print(f'Task {task.status()[\"description\"]} is {task.status()[\"state\"]}') # Print current status of the task (ready or running)\n",
        "            time.sleep(60) # Wait 60 seconds before re-running above\n",
        "        status = task.status()\n",
        "        print(f'Task {status[\"description\"]} completed with status: {status[\"state\"]}') # Print if completed task\n",
        "        if status['state'] != 'COMPLETED': # If not completed\n",
        "            print('Error:', status['error_message'])  # Then print the error associated with this\n",
        "\n",
        "    monitor_task(task)\n",
        "\n"
      ],
      "metadata": {
        "id": "Gq2qVuiK6jzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create household-level indicators\n",
        "Here we take the processed images that we have for each area, load in the points for households, and then estimate the average score for a 300m buffer around each point.\n",
        "\n",
        "I have previously processed the households (UPRNs) so that they are in the neccessary format to help make this next step most efficient here - using `get_uprns_cm.R` (make sure you have run this first). This includes: (i) Using only TOIDs (Topographic Identifers) rather than UPRNs (UPRNs are nested within TOIDs which represent the unique building rather than household - e.g., UPRNs would be the individual flats and the TOID would be the single building). This helps to avoid duplication of estimation for multiple UPRNs in the same location (using TOIDs leads to 23% fewer points to process). (ii)  Calculating a 300m buffer around each point seperately, since this is a computationally expensive task for 1.2M points to do here within this script.\n",
        "\n",
        "We can now estimate the median values of NDVI, EDI and NWDI for all inputs. The code below splits this task up by Local Authority and processes each individually. This was done here because it helps to minimise the memory needs of these operations which can lead to Colab crashing. The code below takes 46 minutes to run in total (about ~7 minutes per Local Authority, depending on their size)."
      ],
      "metadata": {
        "id": "F0WkyuklRNP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that these are all finished, please run the R script `check_toids.R` which will assess whether there are any issues with the files and combine them into a single UPRN level indicator.\n"
      ],
      "metadata": {
        "id": "-k35XXReMS9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert shapely geometries to ee.FeatureCollection\n",
        "def shapely_to_ee_featurecollection(geometries, toid_values):\n",
        "    features = []\n",
        "    for geom, toid in zip(geometries, toid_values):\n",
        "        geo_json = geom.__geo_interface__\n",
        "        ee_geom = ee.Geometry(geo_json)\n",
        "        feature = ee.Feature(ee_geom)\n",
        "        feature = feature.set('TOID', toid)  # Set TOID value\n",
        "        features.append(feature)\n",
        "    return ee.FeatureCollection(features)\n",
        "\n",
        "# Define the mapping of Local Authority codes to image asset IDs\n",
        "la_code_to_image = {\n",
        "    'E06000049': 'Cheshire_East',\n",
        "    'E06000050': 'Chester_and_Cheshire_West',\n",
        "    'E06000006': 'Halton',\n",
        "    'E08000011': 'Knowsley',\n",
        "    'E08000012': 'Liverpool',\n",
        "    'E08000014': 'Sefton',\n",
        "    'E08000013': 'St_Helens',\n",
        "    'E06000007': 'Warrington',\n",
        "    'E08000015': 'Wirral'\n",
        "}\n",
        "\n",
        "# Function to process a single batch of geometries and export to Google Drive\n",
        "def process_batch_and_export(batch_gdf, image, area_name, batch_num, la_code, batch_size):\n",
        "    batch_geometries = batch_gdf['geometry'].tolist()\n",
        "    toid_values = batch_gdf['TOID'].tolist()\n",
        "\n",
        "    ee_fc = shapely_to_ee_featurecollection(batch_geometries, toid_values)\n",
        "\n",
        "    # Apply reducer to the FeatureCollection\n",
        "    reduced_fc = image.reduceRegions(\n",
        "        collection=ee_fc,\n",
        "        reducer=ee.Reducer.median(),\n",
        "        scale=10\n",
        "    )\n",
        "\n",
        "    # Export the results to Google Drive\n",
        "    folder_path = f'Papers/GroundsWell/WP4/Satellites/processed/batches/{la_code}'\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=reduced_fc,\n",
        "        description=f'{area_name}_Vegetation_Water_Indices_Medians_batch_{batch_num}',\n",
        "        folder=folder_path,\n",
        "        fileFormat='CSV'\n",
        "    )\n",
        "    task.start()\n",
        "    print(f'Saved batch {batch_num}')\n",
        "\n",
        "# Function to process a single Local Authority in batches\n",
        "def process_local_authority_in_batches(la_code, batch_size=1000):\n",
        "    area_name = la_code_to_image[la_code]\n",
        "    print(f'Processing Local Authority: {la_code}')\n",
        "\n",
        "    # Load the corresponding image\n",
        "    image = ee.Image(f'projects/ee-groundswelluk/assets/2024/{area_name}_Vegetation_Water_Indices')\n",
        "\n",
        "    # Load the shapefile using geopandas\n",
        "    shapefile_path = f'/content/drive/MyDrive/Papers/GroundsWell/WP4/Satellites/toid_buffers_by_lad/toid_buffer_{la_code}.shp'\n",
        "    gdf = gpd.read_file(shapefile_path)\n",
        "    # gdf = gdf.sample(1000)  # Subset small datasets for testing purposes\n",
        "\n",
        "    # Process in batches\n",
        "    for i in range(0, len(gdf), batch_size):\n",
        "        batch_gdf = gdf.iloc[i:i + batch_size]\n",
        "\n",
        "        # Process batch and export to Google Drive\n",
        "        process_batch_and_export(batch_gdf, image, area_name, i // batch_size + 1, la_code, batch_size)\n",
        "\n",
        "    print('Processing completed!')\n",
        "\n"
      ],
      "metadata": {
        "id": "YLNp73mdUlj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have split the code for each Local Authority individually here so that they can be run independently. It might have been more efficient to place everything into one large loop, but this approach is helpful for any updates after checking the quality of estimates. Just run each code snippet one-by-one to generate the estimates for that particular Local Authority. They each take between 30mins and 1 hour to run (depending on size)."
      ],
      "metadata": {
        "id": "OQGvdLZe78k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process a single Local Authority in batches\n",
        "process_local_authority_in_batches('E08000015')  # Wirral (~ 1 hour)"
      ],
      "metadata": {
        "id": "_GMvAGaMe_EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_local_authority_in_batches('E06000049')  # Cheshire East"
      ],
      "metadata": {
        "id": "aMTKpNYl4Ruv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_local_authority_in_batches('E06000050')  # Chester and Cheshire West"
      ],
      "metadata": {
        "id": "fT9qk7ID4UU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_local_authority_in_batches('E06000006')  # Halton (~25 mins)"
      ],
      "metadata": {
        "id": "KKHw2T_i7o6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_local_authority_in_batches('E08000011')  # Knowsley"
      ],
      "metadata": {
        "id": "zf2h2VBQ7pOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_local_authority_in_batches('E08000012')  # Liverpool"
      ],
      "metadata": {
        "id": "9_uilu5r7pfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_local_authority_in_batches('E08000014')  # Sefton"
      ],
      "metadata": {
        "id": "RypTZMCl7p6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_local_authority_in_batches('E08000013')  # St. Helens"
      ],
      "metadata": {
        "id": "AstIjSoQ7qTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_local_authority_in_batches('E06000007')  # Warrington"
      ],
      "metadata": {
        "id": "SOvqtlGE736n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}